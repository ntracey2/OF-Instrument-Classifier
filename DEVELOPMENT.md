* I am encountering several strange errors with the ofxMaxim library. I believe this is due to the lack of support with the lasted version of visual studio. I have resolved some of the macro issues but there still seems to be a preprocessor error.
* I decided to look into different libraries that would make featuer extraction much simpler than maximilian. I found two specifically that would be great to incorporate into the project, yaafe and Essentia, but unfortunately they are not designed for windows use. While it is possible to use on Windows, they both take a very painful cross-compilation process, and they wouldn't even be compatible with visual studio as they binaries would rely on MinGW. I searched for an equivalent library that I could integrate well into visual studio but could not find anything. I really want to experiment with these specific libraries though, so I spent far too much time playing around with a linux setup. This opened up a whole new world of complications as there seems to be some foundational issues with openframeworks' linux support.
* I decided to switch to the Marsyas library as they provide a precompiled library for windows. The library is complex but contains the features I wish to use in my project. As of now my Visual Studio project is successfully linked to the library.
* After running into far too many problems with the implementation of Marsyas (various dll errors with no helpful debugging resources), I made a final search for libraries that would work for my project. I believe that I have finally found the perfect setup, as I discovered two OpenFrameworks addons that seem to cover what I need for proper feature extraction. The first addon is ofxAudioFile, which simply reads in a WAV or mp3 file and converts it to a time domain representation of the sound in the form of a float array. I am able to pass this array into ofxGist, which is an openframeworks wrapper for the audio analysis library Gist. It takes an array of float samples as an input and has a very simple process of extracting features such as the MFCC, ZCR, and RMS. Currently my program can read in a single WAV file and compute the ZCR, RMS, and Peak Energy value of the signal.
* Currently I am not certain whether the way I am formatting my data will work with the NBC library I intend to use, but I believe this format will be helpful no matter how I implement NBC. I creating methods to collect the feature data for every training file and store them all in a 2D array. This seems to be how input data is formatted in the MLPack examples, but unfortunately they do not have a NBC example so I will have to see how it plays out. My main concern as of now is runtime, as the training array is quite large and the program is going to iterate over nearly 7000 files. I may need to reorganize the way I process the data if this becomes a major problem in the future. I have not ran into any issues with Gist's feature extraction methods, which is very relieving as I may now focus on the machine learning aspect of the project, which I expected to be the more daunting task.
* I created a separate project to set up and experiment with MLPack. I am trying to decide whether using Naive Bayes or a Random Forest model will be better for my dataset. I ran tests with each of them, but unfortunately I am getting pretty low accuracy with both. This is an indicator that I do not have enough features per class, which I somewhat expected at this point. I plan on adding as many features as I can and hopefully that will help my accuracy.
* I am still having some overfitting issues with my dataset. I am starting to wonder if it is a problem with the exact dataset that I am using, as each audio file doesn't follow a strict format. I am looking into experimenting with other datasets, specifically ones that only consist of a single note played per file. I think this will give me a bit more consistency in my data. I also may want to look into an additional library to do pitch calculations, because I discovered that Gist takes a very long time to calculate the pitch for a given audio sample. I tried altering the sample/buffer size of the input, but this didn't seem to help. There are some heavy calulations involved with finding the pitch of an audio buffer, so maybe a more optimized library dedicated to finding pitch will be quicker and more reasonable. I think having pitch data would be benificial to my accuracy as pitch plays an important role in audio classification.
* I have begun to implement the GUI portion of the app while thinking of a solution to my data problem. I am implementing a simple button interface, possibly experimenting with a dropdown menu or text fields from an additional addon, in order to select the configuration for the classification model. I have decided to let the user choose between a set of available model types, consisting of Naive Bayes and Random Forrest as of now. Because MLPack makes it simple to use different classifiers, I think will be a cool addition to my project as it will allow users to see the differences in accuracy between unique classification models on the same dataset. I am still not sure what visuals I am going to incorporate after the classification is complete, but I am leaning towards drawing a confusion matrix or maybe even some graph of the data means/variances.